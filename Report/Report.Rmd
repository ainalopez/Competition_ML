---
title: "Kaggle Report-Team ABBABA OH NO ABBBAB"
author: "Nick Halliwell, Aina Lopez, Yaroslav Marchuk"
date: "March 14, 2016"
output: pdf_document
---


#Introduction
Our team consists of Nick Halliwell, Aina Lopez, and Yaroslav Marchuk. In this competition, we were given both a training and test set consisting of features of various web links from mashable.com. We were asked to predict whether the website link would fall under one of five potential categories: obscure, mediocre, popular, super sopular, and viral. We ran several algorithms to make these multi-class predictions, and found that a random forest with tuned parameters outperformed all other algorithms. Below we underline the approach we took to this competition. 

#Method

**1. Create new variables**  
The first step we took was to examine the data, and determine what kind of variables we could generate from the given data, and standardize our features. Of the variables we were given, we decided that the url of the articles included useful information that could be extracted. An example of the url is the following:

**http://mashable.com/2014/01/12/game-of-thrones-season-4-trailer**

From the url, we took the year, day and month it was published as well as some keywords of the content of the article. Using text mining techniques, we collected all the keywords of the article titles. With this we created new variables indicating whether a specific word appears in the title of the article. Given that we have many keywords, we created a threshold for variables, meaning we created variables only for keywords that have appeared in at least 150 observations.

**2. Feature Selection**  
We used Regularized Random Forest (RRF package) to choose the best feature set. 

**3. Train the model**  
We tried different models, explained in detail below.

**4. Evaluate the model**  
In order to evaluate our models, we used 5-fold Cross validation. Here, we randomly subset our training data into 5 folds, training the model on 4 folds and testing the model on the last fold. This process is repeated 5 times, with each fold used once for training. To stay consistent, we used 5-fold Cross validation on all algorithms used in competition.  


# Models 

* **Random Forest:**  
We decided to implement a regularized Random Forest, as this algorithm typically perform well on large datasets. Random forests often do not overfitting, however, the computational speed of this algorithm can be slow. The regularized Random Forest performed feature selection for us, and then fed those relevant features into a Random Forest. We optimized the algorithm for the number of trees used, the minimum size of terminal nodes, and the number of variables randomly sampled for potential splits. Ultimately this was our most successful model. 


* **K-nearest neighbors:**   
We implemented a k-NN algorithm, trying both a function written by one of us, and an R package installed from CRAN. In both cases, the k-NN performed poorly. In terms of predictions, it's accuracy was lower than most of the other algorithms, and it was very expensive to compute. We ran the algorithm using the distance metric of the Euclidean norm, and optimzed for different values of k, yet the algorithm could not outperform our Random Forest. 

* **SVM**  
In addition to k-NN and Random Forest algorithms, we implemented a Support Vector Machine. We optimized weights for the 5 different classes, giving higher weights to classes 1,2 and 3. The svm was outperformed by the Random Forest, and given the large time complexity of the algorithm, we turned to other methods to use in competition. 

* **Boosting**  



# Choosing Parameters

In order to optimize the parameters of each algortihm, our approach was to create a wide range of reasonable values, very spaced between them. After, we cross-validated all the models and choose the best. Then, we created a new range of values arround the best found value, but this time less spaced. We cross-validated again and choose the best. And so on.

We repeated this procedure as many times as needed.  

Usually, only a couple of rounds of this approach are needed. However, with more complex models we realized that it is very inefficient. We saw that choosing each parameter separetely is not enough, the way of really optimize the parameters is to pick them together. This is one of the main reasons why we decided to don't use Support Vector Machines. 



# Results

We have plot some of the model accuracies (Kaggle public Leaderboard vs our 5-fold Cross Validation). The models that we are displaying are:

* RF: all is the random forest with all the variables, var1 states for the random forest with the variables found using the previously described variable selection method and var2 is the random forest with a different variable selection method (using different parameters in RRF).
* GBM: boosting using the package GBM.
* XGBoost: boosting using the package XGBoost.
* KNN.
* SVM.
* Decision Tree.

Those are a small sample of all the models that we have tried, but represent the best accuracies achieved by each algortihm family. Notice that most of the time we have tried to optimize Random Forest results, because is the one with better performance. 


```{r, echo = FALSE, include = FALSE}

library(ggplot2)

```

```{r,echo = FALSE}

Method <- c("RF: all","RF: var1 (800 trees)","RF: var1 (1000 trees)","RF: var2 (500 trees)","RF: var2 (1200 trees)", "XGBoost",  "Decision Tree", "GBM", "KNN", "SVM" )
CV     <- c(0.5156653, 0.519882, 0.5206152, 0.52125, 0.5217,  0.51909,  0.4635335, 0, 0, 0)
Kaggle <- c(0.53416, 0.55590, 0.55124, 0.54814, 0.53727, 0.53416, 0.48292, 0.54969, 0.49689, 0.53882)


df <- data.frame(Method= Method, CV = CV, Kaggle = Kaggle)


ggplot(df, aes(x = CV, y = Kaggle, color = Method)) + 
  geom_point() +
  ggtitle("Results")  + 
  xlab("5-Fold Cross Validation") + 
  ylab(" Kaggle's Public Leaderboard") + 
  theme_bw()


```



# Conclusions
Sadly, we haven't had time to implement all of our ideas (supersampling, optimizing the seed, choose the Boosting and SVM parameters more carefully). 


As we can see in the figure, the best accuracy in Kaggle's public Leaderboard is not necessarily the same as the best accuracy in our 5-fold CV. For our final submission, we decided to choose the ones with best scores both in CV and in public Leaderboard. 

which one we choose?

Advantages and limitations of our classifier


